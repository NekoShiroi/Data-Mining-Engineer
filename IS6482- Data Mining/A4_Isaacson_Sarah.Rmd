---
title: "A4_Isaacson_Sarah"
author: "Sarah Isaacson"
date: "2024-06-12"
output: 
  html_document:
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: inline
---

```{r library loading}
# Package loading. Install the following packages before running this chunk or knitting this program.
library(C50)
library(knitr)
library(matrixStats)
library(e1071)
library(caret)
library(rminer)
library(tictoc)
library(ggplot2)
library(dplyr)
library(rmarkdown)
library(psych)
library(rpart)
library(RWeka)
library(rpart.plot)
library(rJava)
```

# Task I (95%)
## Part 1. Code chunk 1  (20%)- Set up, data import, data exploration, data partitioning, and inspection code
### A. Package loading, and data import 
#### Set Directory
```{r Set up, data import and inspection }

tic()
# Set the working directory to the directory where your rmarkdown program file resides in rstudio using getwd() and setwd()
mydir_wd <- getwd()
setwd(mydir_wd)

# Import a csv file
GameSales <- read.csv(file = "NA_sales_filtered.csv", stringsAsFactors = FALSE)

```
#### Load character String, Summary, Structure before Factor
```{r structure and summary}
# examine data before data factorization
str(GameSales) #structure
summary(GameSales) #summary
```

#### Other than the Name, transform all other non-numeric fields to be factor variables.
```{r Factors}
#?factor
#?mutate_if
#?across
#?where
#?ifelse
#?name()
GameSales$Platform <- factor(GameSales$Platform) # variable y factorization
GameSales$Genre <- factor(GameSales$Genre) # variable job factorization
GameSales$Rating <- factor(GameSales$Rating) # variable education factorization
```
### B. Use pairs.panels to show distributions and correlations of all of the numeric variables.
```{r pairpanels for distribution}
#?pairs.panels
GameSales %>% select(where(is.numeric)) %>% pairs.panels()
```
### C. Remove the Name variable from the data frame. All subsequent models should have this column excluded. Build a linear regression model. Show the summary of the model to understand the significance and coefficients of the predictors in the model and the overall model fit. Note that the purpose of this task is not to build a predictive model. Rather, it is often a good idea to explore a data set with white-box models like linear regression (for numeric target variable) or decision tree (for factor target variable).
```{r removing name column}
#remove the Name Column
GameSales <- GameSales %>% select(-Name)
```
```{r build linear regression model 1}
#regression model
Regression_Model <- lm(NA_Sales ~ ., data = GameSales)
Regression_Model
summary(Regression_Model)
```
### D. Partition the dataset for simple hold-out evaluation – 70% for training and the other 30% for testing.
```{r DataPartition}
set.seed(100)
index <- createDataPartition(GameSales$NA_Sales,p=.7,list = FALSE)
train_target <- GameSales[index,8]
test_target <- GameSales[-index,8]
train_set <- GameSales[index,-8] #partition the 70% to the train set 
test_set <- GameSales[-index,-8] #simply get the rest of the set and make it test set (30% from the 100%)
```
### E. Show the overall summaries of training and testing sets.
```{r overall training and testing}
summary(train_set)
summary(test_set)
```
## Part 2. Code chunk 2 (20%)– lm, rpart and M5P model training and testing
### A. Train three models using lm, rpart, and M5P on the training set (built in 1. D). Use the default settings of these methods throughout this assignment.
```{r lm,rpart,M5P model building}
#linear regression model
gs_lm_train_model <- lm(train_target~., data = train_set)
gs_lm_train_model
#rpart model
gs_rpart_model <- rpart(train_target ~ ., data = train_set)
gs_rpart_model
#M5P model
gs_m5p_model <- M5P(train_target ~ ., data = train_set)
gs_m5p_model
```
### B. For each of the three models trained in 2.A, perform the following:
#### ii) Apply the model and generate the model-fit (R2) and prediction error metrics (MAE, MAPE, RAE, RMSE, RMSPE, RRSE)  in both the testing and training sets.
##### Linear Regression
```{r model fit and prediction error metrics for linear regression}
#metric list
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")
#apply prediction
pred_lm_train <- predict(gs_lm_train_model,train_set)
pred_lm_test <- predict(gs_lm_train_model,test_set)

mmetric(test_target,pred_lm_test,metrics_list)
mmetric(train_target,pred_lm_train,metrics_list)
```
##### RPART
```{r model fit and prediction error metrics for rpart}
#apply prediction
pred_rpart_train <- predict(gs_rpart_model,train_set)
pred_rpart_test <- predict(gs_rpart_model,test_set)

mmetric(test_target,pred_rpart_test,metrics_list)
mmetric(train_target,pred_rpart_train,metrics_list)
```
##### M5P
```{r model fit and prediction error metrics for M5P}
#apply prediction
pred_m5p_train <- predict(gs_m5p_model,train_set)
pred_m5p_test <- predict(gs_m5p_model,test_set)

mmetric(test_target,pred_m5p_test,metrics_list)
mmetric(train_target,pred_m5p_train,metrics_list)
```
## Part 3. Code chunk 3 (20%) – Cross-validation of lm, rpart, and M5P NA_Sales prediction models
### A. Define a named function for cross-validation of numeric prediction models that generates a table of the model fit and error metrics specified in 2.B for each fold along with the means and standard deviations of the metrics over all of the folds.
```{r define cv_function}

cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
  # create folds
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  # perform cross validation
  cv_results <- lapply(folds, function(x)
  { 
    test_target <- df[x,target]
    test_input  <- df[x,-target]

    train_target <- df[-x,target]
    train_input <- df[-x,-target]

    prediction_model <- prediction_method(train_target~.,train_input) 
    pred<- predict(prediction_model,test_input)
    return(mmetric(test_target,pred,metrics_list))
  })
  # generate means and sds and show cv results, means and sds using kable
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_mean) <- "Mean"
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  kable(t(cv_all),digits=2)
}
```
### B. Call the function in 3.A to generate 5-fold cross-validation results of lm, rpart and M5P models for NA_sales.
```{r call functions in 3.A}

df <- GameSales
target <- 8
nFolds <- 5
seedVal <- 123
# 5-folds for lm 
prediction_method<- lm
lm_cv_function <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
lm_cv_function
# 5-folds for rpart
prediction_method<- rpart
rpart_cv_function <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
rpart_cv_function
# 5-folds for m5p
prediction_method<- M5P
m5p_cv_function <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
m5p_cv_function
```
## Part 4. Code chunk 4 (20%) – Improve the models by adding a quadratic term of Critic_Score
### A. Create and add the quadratic term of Critic_Score, e.g., Critic_Score_Squared, to the predictors for NA_Sales in the whole data set for this assignment.
```{r Create and add the quadratic term of Critic_Score, e.g., Critic_Score_Squared}

# Create a copy of the 'data' dataframe
gs_data_transformed <- GameSales %>% mutate(Critic_Score_Squared = (Critic_Score)^2)



```
### B. Build an lm model using the whole data set that includes Critic_Score_Squared to predict NA_Sales. Show the summary of this lm model. This allows you to inspect if this squared term is significant or not.
```{r Show the summary of this lm model. This allows you to inspect if this squared term is significant or not.}



# transformed model
lm_model_transformed <- lm(NA_Sales ~., data = gs_data_transformed)


# Summary of lm transformed model
summary(lm_model_transformed)

```
### C. Call the cross-validation function defined for 3.A to generate 5-fold cross-validation results of the lm, rpart and M5P models with Critic_Score_Squared.
```{r call functions in 4.C}
df <- gs_data_transformed
target <- 8
nFolds <- 5
seedVal <- 500
# 5-folds for lm 
prediction_method<- lm
lm_cv_function_transformed <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
lm_cv_function_transformed
# 5-folds for rpart
prediction_method<- rpart
rpart_cv_function_transformed <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
rpart_cv_function_transformed
# 5-folds for m5p
prediction_method<- M5P
m5p_cv_function_transformed <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
m5p_cv_function_transformed
```
## Part 5. Code chunk 5 (12%) – Improve the models with the log term of User_Count:
### A. Create and add the natural log transformation of User_Count, e.g., log_User_Count, to the predictors for the target variable.  The following is an excerpt of sample code in webinar's demo:

Remove the original User_Count (7th column) and create a new data frame

df_log_User_Count <- sales[,-7]

Create and add the natural log transformation of User_Count
df_log_User_Countlog_User_Count <- log(salesUser_Count)

```{r removing stuff}
# Remove the original User_Count (7th column) and create a new data frame

df_log_User_Count <- GameSales[,-7]

# Create and add the natural log transformation of User_Count
df_log_User_Count$log_User_Count <- log(GameSales$User_Count)
```
### B. Build an lm model with the whole data set that includes log_User_Count and excludes User_Count. The input data should not include any quadratic terms created in the previous code chunk. Show the summary of this lm model. This allows you to inspect if this log term is significant or not.
```{r lm model in 5.B}
lm_5b_model <- lm(NA_Sales ~., data= df_log_User_Count)
summary(lm_5b_model)
```
### C. Call the cross-validation function defined for 3.A to generate 5-fold cross-validation results of the lm, rpart, and M5P models with log_User_Count included and User_Count excluded.
```{r cross-validation 5.C}

df <- df_log_User_Count
target <- 7
nFolds <- 5
seedVal <- 500
metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")
# 5-folds for lm 
prediction_method<- lm
lm_cv_function_log <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
lm_cv_function_log
# 5-folds for rpart
prediction_method<- rpart
rpart_cv_function_log <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
rpart_cv_function_log
# 5-folds for m5p
prediction_method<- M5P
m5p_cv_function_log <- cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
m5p_cv_function_log
```
# Task II Reflections (8%):

## 1. Which predictor would you recommend to remove to keep the models more parsimonious? Give your reasons based on supporting empirical information from data exploration and/or model summary and performance. (parsimonious in this context means using as few predictors as possible while still maintaining high quality in the model) 
 

## 2. What are the reasons why the log of User_Count could be more effective than User_Count in predicting sales based on both model fit and prediction error measures? Provide supporting empirical evidence including information from data exploration and model performance for your reasons.



## 3. In addition to adding the quadratic term of Critic_Score to the predictors of a linear regression model, would you recommend adding the quadratic term of User_Count to the model also? Explain the reason for your recommendation. Provide supporting empirical information from data exploration and/or model performance.


## 4. What have you learned from building each of these models and the modeling impact of your adjustments to the hyperparameters or dataset? If you were explaining the results of these models to a supervisor what would you say about them? Attempt to do more than just state facts here, interpret the results. Coding is great, interpretation of output is even more important. Discuss each model.  Write at least 150 words.


# Additional Questions:

## 1. Why can transforming variables improve performance in the models used in this
assignment?



## 2. Do you think an interaction term could have improved performance? Which variable relationships might be worth exploring? 



## 3. How were factors implemented into your regressions? Is the number of coefficients the same as the number of levels? If not, why are they different?


