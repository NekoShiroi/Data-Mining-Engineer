---
title: "Video Game Sales Prediction using Blackbox Methods"
author: "Meenakshi"
date: "22/06/2024"
output:
  html_document:
    number_sections: yes
    toc: yes
    fig_width: 15
    fig_height: 10
  word_document:
    toc: yes
editor_options:
  chunk_output_type: inline
---


# Package Load, Data Import, Inspection, and Partitioning

## Loading Required Packages

```{r}
library(caret)
library(RWeka)
library(kernlab)
library(rminer)
library(matrixStats)
library(knitr)
library(tictoc)
library(ggplot2)
library(dplyr)
library(rmarkdown)
library(tidyverse)
```

## Importing the Data and Partitioning the Dataset

```{r}

# Importing the dataset
na_sales <- read.csv("NA_sales_filtered.csv", stringsAsFactors = FALSE)

# Excluding the 'Name' column
na_sales <- na_sales[ , !(names(na_sales) %in% c('Name'))]

# Transforming character variables to factors
na_sales[sapply(na_sales, is.character)] <- lapply(na_sales[sapply(na_sales, is.character)], as.factor)

# Partitioning the dataset: 70% training and 30% testing
set.seed(123)
trainIndex <- createDataPartition(na_sales$NA_Sales, p = .7, list = FALSE, times = 1)
na_train <- na_sales[trainIndex,]
na_test <- na_sales[-trainIndex,]
summary(na_sales)
summary(na_sales)

```


# Building and Evaluating Neural Network Models
## MLP Model with Default Settings

```{r}

MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
# Building the MLP model with default settings
mlp_model_default <- MLP(NA_Sales ~ ., data = na_train)
metrics<-c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")
# Evaluating the model performance on the training set
mlp_train_pred_default <- predict(mlp_model_default, na_train)
mlp_test_pred_default <- predict(mlp_model_default, na_test)

train_eval<-mmetric(na_train$NA_Sales,mlp_train_pred_default,metrics)
test_eval<-mmetric(na_test$NA_Sales,mlp_test_pred_default,metrics)
train_eval
test_eval
# Calculating performance metrics
#mlp_train_perf_default <- postResample(mlp_train_pred_default, na_train$NA_Sales)
#mlp_test_perf_default <- postResample(mlp_test_pred_default, na_test$NA_Sales)

#list(mlp_test_pred_default = train_rmse, Train_MAE = train_mae,
 #    mlp_test_pred_default = test_rmse, Test_MAE = test_mae)

```

## MLP Model with Two Hidden Layers and Modified Hyperparameters

```{r}
# Building MLP model with two hidden layers and modified learning rate
mlp_model_tuned <- MLP(NA_Sales ~ ., data = na_train, control = Weka_control(H = 'a,a', L = 0.1))

# Evaluating the model performance on the training set
mlp_train_pred_tuned <- predict(mlp_model_tuned, na_train)
mlp_test_pred_tuned <- predict(mlp_model_tuned, na_test)

train_eval<-mmetric(na_train$NA_Sales,mlp_train_pred_tuned,metrics)
test_eval<-mmetric(na_test$NA_Sales,mlp_test_pred_tuned,metrics)

train_eval
test_eval
# Calculating performance metrics
#mlp_train_perf_tuned <- postResample(mlp_train_pred_tuned, na_train$NA_Sales)
#mlp_test_perf_tuned <- postResample(mlp_test_pred_tuned, na_test$NA_Sales)

#list(mlp_test_pred_default = train_rmse, Train_MAE = train_mae,
    # mlp_test_pred_default = test_rmse, Test_MAE = test_mae)

```


# Building and Evaluating SVM Models

## SVM Model with Default Settings

```{r}

# Building SVM model with default settings
svm_model_default <- ksvm(NA_Sales ~ ., data = na_train)

# Evaluating the model performance on the training set
svm_train_pred_default <- predict(svm_model_default, na_train)
svm_test_pred_default <- predict(svm_model_default, na_test)

train_eval<-mmetric(na_train$NA_Sales,svm_train_pred_default,metrics)
test_eval<-mmetric(na_test$NA_Sales,svm_test_pred_default,metrics)

train_eval
test_eval

# Calculating performance metrics
#svm_train_perf_default <- postResample(svm_train_pred_default, na_train$NA_Sales)
#svm_test_perf_default <- postResample(svm_test_pred_default, na_test$NA_Sales)

# Output the RMSE results
#list(svm_train_perf_default = svm_train_perf_default, svm_test_perf_default = svm_test_perf_default)

```

## SVM Model with Different Kernel Function

```{r}

# Build SVM model with a different kernel function
svm_model_rbf <- ksvm(NA_Sales ~ ., data = na_train, kernel = 'rbfdot')

# Evaluate the model performance on the training set
svm_train_pred_rbf <- predict(svm_model_rbf, na_train)
svm_test_pred_rbf <- predict(svm_model_rbf, na_test)

train_eval<-mmetric(na_train$NA_Sales,svm_train_pred_rbf,metrics)
test_eval<-mmetric(na_test$NA_Sales,svm_test_pred_rbf,metrics)

train_eval
test_eval

# Calculate performance metrics
#svm_train_perf_rbf <- postResample(svm_train_pred_rbf, na_train$NA_Sales)
#svm_test_perf_rbf <- postResample(svm_test_pred_rbf, na_test$NA_Sales)

# Output theresults
#list(svm_train_perf_rbf = svm_train_perf_rbf, #svm_test_perf_rbf = svm_test_perf_rbf)


```

## SVM Model with Modified Cost Value

```{r}

# Build SVM model with a different cost value
svm_model_cost <- ksvm(NA_Sales ~ ., data = na_train, C = 5)

# Evaluate the model performance on the training set
svm_train_pred_cost <- predict(svm_model_cost, na_train)
svm_test_pred_cost <- predict(svm_model_cost, na_test)

train_eval<-mmetric(na_train$NA_Sales,svm_train_pred_cost,metrics)
test_eval<-mmetric(na_test$NA_Sales,svm_test_pred_cost,metrics)

train_eval
test_eval
# Calculate performance metrics
#svm_train_perf_cost <- postResample(svm_train_pred_cost, na_train$NA_Sales)
#svm_test_perf_cost <- postResample(svm_test_pred_cost, na_test$NA_Sales)

# Output the RMSE results
#list(svm_train_perf_cost =svm_train_perf_cost, svm_test_perf_cost=svm_test_perf_cost)


```


# Building and Evaluating KNN Models

## KNN Model with Default Settings

```{r}


# Build KNN model with default settings
knn_model_default <- IBk(NA_Sales ~ ., data = na_train)

# Evaluate the model performance on the training set
knn_train_pred_default <- predict(knn_model_default, na_train)
knn_test_pred_default <- predict(knn_model_default, na_test)

train_eval<-mmetric(na_train$NA_Sales,knn_train_pred_default,metrics)
test_eval<-mmetric(na_test$NA_Sales,knn_test_pred_default,metrics)

train_eval
test_eval

# Calculate performance metrics
#knn_train_perf_default <- postResample(knn_train_pred_default, na_train$NA_Sales)
#knn_test_perf_default <- postResample(knn_test_pred_default, na_test$NA_Sales)

#list(knn_train_perf_default = knn_train_perf_default, knn_test_perf_default = knn_test_perf_default)

```


## KNN Model with Different K Value

```{r}

# Build KNN model with K = 5
knn_model_k5 <- IBk(NA_Sales ~ ., data = na_train, control = Weka_control(K = 5))

# Evaluate the model performance on the training set
knn_train_pred_k5 <- predict(knn_model_k5, na_train)
knn_test_pred_k5 <- predict(knn_model_k5, na_test)

train_eval<-mmetric(na_train$NA_Sales,knn_train_pred_k5,metrics)
test_eval<-mmetric(na_test$NA_Sales,knn_test_pred_k5,metrics)

train_eval
test_eval
# Calculate performance metrics
#knn_train_perf_k5 <- postResample(knn_train_pred_k5, na_train$NA_Sales)
#knn_test_perf_k5 <- postResample(knn_test_pred_k5, na_test$NA_Sales)

#list(knn_train_perf_k5 = knn_train_perf_k5, knn_test_perf_k5 = knn_test_perf_k5)

```

# KNN Model with Weighted Voting

```{r}

# Build KNN model with weighted voting
knn_model_weighted <- IBk(NA_Sales ~ ., data = na_train, control = Weka_control(I = TRUE))

# Evaluate the model performance on the training set
knn_train_pred_weighted <- predict(knn_model_weighted, na_train)
knn_test_pred_weighted <- predict(knn_model_weighted, na_test)

train_eval<-mmetric(na_train$NA_Sales,knn_train_pred_weighted,metrics)
test_eval<-mmetric(na_test$NA_Sales,knn_test_pred_weighted,metrics)
train_eval
test_eval

# Calculate performance metrics
#knn_train_perf_weighted <- postResample(knn_train_pred_weighted, na_train$NA_Sales)
#knn_test_perf_weighted <- postResample(knn_test_pred_weighted, na_test$NA_Sales)

#list(knn_train_perf_weighted = knn_train_perf_weighted, knn_test_perf_weighted = knn_test_perf_weighted)

```

# KNN Model with Automatic K Selection

```{r}

# Build KNN model with automatic K selection
knn_model_auto <- IBk(NA_Sales ~ ., data = na_train, control = Weka_control(X = TRUE))

# Evaluate the model performance on the training set
knn_train_pred_auto <- predict(knn_model_auto, na_train)
knn_test_pred_auto <- predict(knn_model_auto, na_test)

train_eval<-mmetric(na_train$NA_Sales,knn_train_pred_auto,metrics)
test_eval<-mmetric(na_test$NA_Sales,knn_test_pred_auto,metrics)

train_eval
test_eval
# Calculate performance metrics
#knn_train_perf_auto <- postResample(knn_train_pred_auto, na_train$NA_Sales)
#knn_test_perf_auto <- postResample(knn_test_pred_auto, na_test$NA_Sales)

#list(knn_train_perf_auto = knn_train_perf_auto, knn_test_perf_auto = knn_test_perf_auto)

```


# Cross-validation Function for Numeric Prediction Models

```{r}

# it's for 3 models
cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
# create folds using the assigned values

set.seed(seedVal)
folds = createFolds(df[,target],nFolds)

  # The lapply loop
  
  cv_results <- lapply(folds, function(x)
{ 
# data preparation:
  test_target <- df[x,target]
  test_input <- df[x,-target]
  
  train_target <- df[-x,target]
  train_input <- df[-x,-target]
  pred_model <- prediction_method(train_target ~ .,data = train_input)  
  pred <- predict(pred_model, test_input)
  return(mmetric(test_target,pred,metrics_list))
})

cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}

```

# 3-Fold Cross-Validation of MLP, SVM and KNN Models

## MLP Model Cross-Validation

```{r}
# Define the ksvm model function
# Define some example metrics

cv_function(na_sales, 8, 3, 500, MLP, metrics)
cv_function(na_sales, 8, 3, 500, ksvm,metrics)
cv_function(na_sales, 8, 3, 500, IBk, metrics)



#mlp_cv_results <- cv_function(na_sales, "NA_Sales", 3, 123, "mlp", metrics_list)



```

```

## KNN Model Cross-Validation




# Discussions
## KSVM Model Performance
### 1. Parameter Changes and Impact on Performance Metrics
* Changing the kernel function from linear to radial basis (RBF) generally improves the model's ability to capture non-linear relationships in the data, resulting in better performance metrics such as lower MAE and RMSE.
* Increasing the cost parameter (C) tends to make the model more complex, which can improve performance on the training set but may lead to overfitting if the value is too high.

## MLP Model Performance
### 2. Parameter Changes and Impact on Performance Metrics
* Modifying the learning rate can significantly impact the convergence of the model. A lower learning rate may lead to better convergence but requires more epochs, while a higher learning rate can speed up training but might overshoot the optimal solution.
* Adding more hidden layers or nodes increases the model's capacity, which can improve performance metrics if the data has complex patterns. However, it also increases the risk of overfitting and computational cost.

### 3. Learnings from Model Building 
* Building each model helps in understanding the strengths and weaknesses of different algorithms. MLP models are powerful for capturing complex patterns but require careful tuning of hyperparameters.
* SVM models are robust and effective for both linear and non-linear data but can be computationally intensive for large datasets.
* KNN models are simple and intuitive but might not perform well for high-dimensional data and can be slow for large datasets due to the need to compute distances between all points.

# Additional Questions

## IBk Model Performance

* Changing the K value affects the bias-variance trade-off. A lower K value may capture noise (high variance), while a higher K value may oversimplify (high bias). Weighted voting can improve predictions by giving more importance to closer neighbors.

## Runtime Differences

* IBk can be slower compared to SVM and MLP due to its instance-based nature, requiring computation of distances for all data points. SVM and MLP, once trained, can make predictions relatively quickly.

## Emergency Prediction

* In an urgent situation, SVM with a simple linear kernel or a default KNN model can be quickly deployed. These models generally require less hyperparameter tuning compared to MLP and can provide reasonably accurate predictions with default settings.


